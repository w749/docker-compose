input {
  file {
    path => ["/usr/share/logstash/data/file/CouponProduct.csv"]
    # sincedb记录现在有一个与之关联的最后活动时间戳。如果在最近N天内没有在跟踪文件中检测到任何更改，则它的sincedb跟踪记录将过期，并且不会被持久保存。默认14天
    sincedb_clean_after => "2 weeks"
    sincedb_path => "/usr/share/logstash/data/file/sincedb"  # sincedb存储地址
    start_position => "beginning"  # 选择Logstash最初开始读取文件的位置:开始或结束。默认行为将文件视为实时流（end），因此从末尾开始。
  }
}
filter{
  csv {
    separator => ","  # 分隔符，默认是逗号
    # quote_char => "\""  # 定义用于引用CSV字段的字符，默认是双引号
    columns => ["id", "code", "name", "enname", "points", "creater", "updater", "money", "createtime", "updatetime"]  # 指定列名
    autodetect_column_names => false  # 是否检测列名，默认为false
    skip_header => false  # 是否跳过第一行，和autodetect_column_names一起设置，全为true或false
    autogenerate_column_names => false  # 是否设置默认的列名，默认为true
    convert => {  # 指定数据类型
      "money" => "integer"
      "createtime" => "date"
      "updatetime" => "date"
    }
    skip_empty_columns => false  # 跳过空列，默认false
    skip_empty_rows => false  # 跳过空行，默认false
  }
  mutate {
    add_field => {"insert_time" => "%{@timestamp}"}  # 新增字段，%{已有字段}可以引用现有字段
    remove_field => ["message","@version","updater","host","@timestamp"]  # 删除字段
  }
}
output {
  stdout{
  }
  elasticsearch {
    hosts => ["node1:9200","node2:9201"]  # 输出到ES，和logstash.yml保持一致
    index => "coupon_product"  # 索引名称
  }
}
